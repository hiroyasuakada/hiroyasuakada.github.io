<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Hiroyasu Akada</title>
  <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
  <meta name="author" content="Hiroyasu Akada">
  <meta name="description" content="Hiroyasu Akada's Homepage">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords" content="
  Egocentric, 3D, Human, Pose Estimation,
  Bring Your Rear Cameras for Egocentric 3D Human Pose Estimation,
  3D Human Pose Perception from Egocentric Stereo Videos,
  UnrealEgo,
  UnrealEgo2,
  EventEgo3D,
  Computer Vision,
  Deep Learning,
  ">

  <!-- Canonical URL -->
  <link rel="canonical" href="https://hakada.github.io/">

  <!-- Open Graph / Facebook -->
  <meta property="og:title" content="Hiroyasu Akada">
  <meta property="og:description"
    content="Hiroyasu Akada's Homepage - Ph.D. student at Max Planck Institute for Informatics, specializing in Computer Vision and Deep Learning.">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://hakada.github.io/">
  <meta property="og:image" content="https://hakada.github.io/images/profile_photo.jpg">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Hiroyasu Akada">
  <meta name="twitter:description"
    content="Hiroyasu Akada's Homepage - Ph.D. student at Max Planck Institute for Informatics, specializing in Computer Vision and Deep Learning.">
  <meta name="twitter:image" content="https://hakada.github.io/images/profile_photo.jpg">

  <!-- <meta name="google-site-verification"
      content="googlef77166d9f74b3a8e.html"> -->
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="XXX"> -->
  <script type="text/javascript">
    function load_text(filename, id) {
      window.addEventListener('DOMContentLoaded', function () {
        fetch(filename)
          .then(response => response.text())
          .then(data => {
            const file_area = document.getElementById(id);
            file_area.innerHTML = data.replace(/\n/g, "<br>");
            //file_area.innerHTML = file_area.innerHTML.replace(/\s+/g, "&emsp;");
            file_area.innerHTML = file_area.innerHTML.replace(/\s+/g, "&nbsp;");
          });
      });
    };
  </script>
</head>

<body>
  <table
    style="width:100%;max-width:950px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Hiroyasu Akada (朱田 浩康)</name>
                  </p>
                  <p style="font-size: 1.5em;">Hi, I am <b>Hiroyasu Akada</b> from Japan <img src="./images/sushi.png"
                      width="18"> <img src="./images/ramen.png" width="18"> <img src="./images/bbq.png" width="18"> <img
                      src="./images/takoyaki.png" width="18"> <img src="./images/shrimp.png" width="18">
                    .</p>
                  <p>
                    I am a Ph.D. candidate in the Visual Computing and Artificial Intelligence Department at <a
                      href="https://www.mpi-inf.mpg.de/departments/visual-computing-and-artificial-intelligence">Max
                      Planck Institute for Informatics</a>
                    with my supervisor Dr. <a href="https://4dqv.mpi-inf.mpg.de/">Vladislav Golyanik</a> and Prof. <a
                      href="https://people.mpi-inf.mpg.de/~theobalt/">Christian Theobalt</a>.
                  </p>
                  <!-- <p>
                    I am currently interning at Google as a student researcher in Zurich, Switzerland, working on
                    synthetic data generation and generative AI.
                  </p> -->
                  <p>
                    Previously, I obtained my MS and BS at Keio University, with Prof. <a
                      href="https://www.st.keio.ac.jp/en/tprofile/sd/takahashi.html">Masaki Takahashi</a>.
                    During my MS, I had a great opportunity to work with Prof. <a href="https://peterwonka.net/">Peter
                      Wonka</a> at <a href="https://www.kaust.edu.sa/en">KAUST</a>. <br>
                    Also, I went to <a href="https://haas.berkeley.edu/">University of California, Berkeley</a> to study
                    Business Administration and spent a great time at <a
                      href="https://www.tencent.com/en-us/">Tencent</a> in California, USA as an intern.
                  </p>

                  <p style="text-align:left">
                    E-mail: hakada@mpi-inf.mpg.de<br>
                    <a href="https://scholar.google.com/citations?user=9Kw95JgAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://twitter.com/hiroyasu_akada">Twitter</a>&nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/hiroyasu-akada-a469b716a">Linkedin</a>&nbsp/&nbsp
                    <a href="https://github.com/hiroyasuakada">Github</a>
                    <!-- &nbsp/&nbsp -->
                    <!-- <a href="https://github.com/xxxx/">Github</a> -->
                  </p>
                </td>
                <!-- Profile image -->
                <td style="padding:0.5%;width:40%;max-width:40%">
                  <a href="images/profile_photo.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/profile_photo.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>


          </table>

          <!-- Image Gallery Section -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <div
                    style="display:flex;justify-content:space-between;align-items:flex-start;gap:10px;flex-wrap:wrap;">

                    <div style="flex:1;min-width:150px;text-align:center;">
                      <img src="./images/mpi.png" alt="Max Planck Institute for Informatics"
                        style="width:100%;max-width:250px;height:60px;object-fit:contain;border-radius:8px;">
                      <p style="margin-top:0px;font-size:14px;color:#666;">MPI-INF <br>
                        (2022-)</p>
                    </div>

                    <div style="flex:1;min-width:150px;text-align:center;">
                      <img src="./images/google.png" alt="Google"
                        style="width:100%;max-width:250px;height:60px;object-fit:contain;border-radius:8px;">
                      <p style="margin-top:0px;font-size:14px;color:#666;">Google <br> (2025)</p>
                    </div>

                    <div style="flex:1;min-width:150px;text-align:center;">
                      <img src="./images/keio.png" alt="Image 3"
                        style="width:100%;max-width:250px;height:60px;object-fit:contain;border-radius:8px;">
                      <p style="margin-top:0px;font-size:14px;color:#666;">Keio University <br> (2014-2022)</p>
                    </div>

                    <div style="flex:1;min-width:150px;text-align:center;">
                      <img src="./images/ucberkeley.png" alt="Image 3"
                        style="width:100%;max-width:250px;height:60px;object-fit:contain;border-radius:8px;">
                      <p style="margin-top:0px;font-size:14px;color:#666;">UC Berkeley <br> (2018-2019)</p>
                    </div>

                    <div style="flex:1;min-width:150px;text-align:center;">
                      <img src="./images/tencent.png" alt="Image 3"
                        style="width:100%;max-width:250px;height:60px;object-fit:contain;border-radius:8px;">
                      <p style="margin-top:0px;font-size:14px;color:#666;">Tencent <br>(2019)</p>
                    </div>

                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">



    </tbody>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>News</heading>
          <p>
            <b>[July 2025]</b> I'll start a student researcher position at Google Zurich(2025.09~).
            <br>
            <b>[June 2025]</b> One paper got accepted to ICCV 2025. Stay tuned for more details!
            <br>
            <b>[May 2025]</b> Selected as an <b style="color:red">outstanding reviewer</b> for CVPR 2025.
            <br>
            <b>[May 2024]</b> Two main conference papers were invited to a poster session in <a
              href="https://egovis.github.io/cvpr24/">EgoVis workshop</a> at CVPR 2024.
            <br>
            <b>[Feb 2024]</b> Two papers got accepted to CVPR 2024 (<b style="color:red">Highlight</b> and
            Poster).
            <br>
            <b>[Oct 2022]</b> Paper <a href="https://4dqv.mpi-inf.mpg.de/UnrealEgo/">"UnrealEgo"</a> was invited
            to a spotlight session in <a href="https://ego4d-data.org/workshops/eccv22/">Ego4D workshop</a> and
            a poster session in <a href="https://sites.google.com/view/egocentric-hand-body-activity">HBHA
              workshop</a> at ECCV 2022.
            <br>
            <b>[Aug 2022]</b> I'm pleased to announce that I received the Nakajima Foundation Scholarship with
            <b style="color:red"> financial support for my PhD study (5 years)</b>, including tuition fees,
            living expenses, monthly stipends, etc.
            <br>
            <b>[Jul 2022]</b> Paper <a href="https://4dqv.mpi-inf.mpg.de/UnrealEgo/">"UnrealEgo"</a> got
            accepted to ECCV 2022.
            <br>
            <b>[Oct 2021]</b> Paper <a
              href="https://openaccess.thecvf.com/content/WACV2022/html/Akada_Self-Supervised_Learning_of_Domain_Invariant_Features_for_Depth_Estimation_WACV_2022_paper.html">Self-Supervised
              Learning of Domain Invariant Features for Depth Estimation</a> got accepted to WACV 2022.
            <br>
            <b>[May 2021]</b> Paper <a href="https://link.springer.com/chapter/10.1007/978-3-030-95892-3_48">Dynamic
              Object Removal from
              Unpaired Images for Autonomous Agricultural Robots</a> got accepted to IAS-16.
            <br>
            <!-- <details>
                        <summary>Past updates</summary>
                      </details> -->
          </p>

        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Research</heading>
          <p>
            My research focus lies at the intersection of Computer Vision and Deep Learning.
            In particular, I am interested in the following topics:

          <ul>
            <li>Egocentric 3D Vision</li>
            <li>Human Pose Estimation</li>
            <li>Human Motion Generation (Diffusion Model)</li>
            <li>Human Body Reconstruction with Self-Supervised Personalization from Images and Speech (LLM)</li>
            <li>Synthetic-to-Real Domain Adaptation</li>
          </ul>
          <!-- <a href="publications.html" target="_blank">Full list of my publications</a> is available. -->
          </p>

          <br>
          <a href="images/teaser.png"><img style="width:90%;max-width:90%;" alt="teaser" src="images/teaser.png"
              class="hoverZoomLink"></a>

        </td>
      </tr>

    </tbody>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">



    <table
      style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
          </td>
        </tr>
      </tbody>
    </table>
    <table
      style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">



      <tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src="./projects/iccv2025_egorear/overview.png" width="180">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://4dqv.mpi-inf.mpg.de/EgoRear/">
            <h3 class="papertitle">
              <strong>Bring Your Rear Cameras for Egocentric 3D Human Pose Estimation</strong>
            </h3>
          </a>
          <b>Hiroyasu Akada</b>, Jian Wang, Vladislav Golyanik, and Christian Theobalt
          <br>
          <em>International Conference on Computer Vision (ICCV)</em>, 2025
          <br>
          <a href="https://4dqv.mpi-inf.mpg.de/EgoRear/">[Project page]</a>
          <a href="https://arxiv.org/abs/2503.11652">[Paper]</a>
          <a href="https://github.com/hiroyasuakada/EgoRear">[Code (coming soon)]</a>
          <p>
            Egocentric 3D full-body tracking has been studied using cameras installed in front of a head-mounted
            device (HMD).
            While frontal placement is the optimal and the only option for some tasks, such as hand tracking, it
            remains unclear if the same holds for full-body tracking.
            Notably, even the state-of-the-art methods often fail to estimate accurate 3D poses in many
            scenarios, such as when HMD users tilt their heads upward---a common motion in human activities.
            Hence, this paper investigates the usefulness of rear cameras in the HMD design for full-body
            tracking.
            We show that simply adding rear views to the frontal inputs is not optimal for existing methods and
            propose a new method that refines 2D joint heatmap estimation, thereby significantly improving 3D
            pose tracking (>10% on MPJPE).
            Furthermore, we introduce two new large-scale datasets, Ego4View-Syn and Ego4View-RW, for a
            rear-view evaluation.
          </p>
        </td>
      </tbody>


      <tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src="projects/cvpr2024_eventego3d/eventego3d.png" width="180">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://eventego3d.mpi-inf.mpg.de">
            <h3 class="papertitle">
              <strong>EventEgo3D++: 3D Human Motion Capture from a Head Mounted Event Camera</strong>
            </h3>
          </a>
          Christen Millerdurai, <b>Hiroyasu Akada</b>, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav
          Golyanik
          <br>
          <em>International Journal of Computer Vision (IJCV)</em>, 2025
          <br>
          <a href="https://eventego3d.mpi-inf.mpg.de">[Project page]</a>
          <a href="https://arxiv.org/abs/2502.07869">[Paper]</a>
          <a href="https://github.com/Chris10M/EventEgo3D_plus_plus">[Code (coming soon)]</a>
          <p>
            This paper is an extension of our previous work "EventEgo3D (CVPR 2024)" (See below).
            We tackle a new problem, i.e. 3D human motion capture from an egocentric monocular event camera with
            a
            fisheye lens.
            "EventEgo3D (CVPR 2024)" proposed the EE3D framework that is specifically tailored for learning with
            event streams in the LNES
            representation, enabling high 3D reconstruction accuracy.
            We upgrade this framework to a new EE3D++ framework for further performance improvement.
            We also introduce a new dataset, EE3D-W, in addition to EE3D-S and EE3D-R from "EventEgo3D (CVPR
            2024)".
          </p>
        </td>
      </tbody>


      <tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src="projects/cvpr2024_unrealego2/overview_dataset.png" width="180">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://4dqv.mpi-inf.mpg.de/UnrealEgo2/">
            <h3 class="papertitle">
              <strong>3D Human Pose Perception from Egocentric Stereo Videos</strong>
            </h3>
          </a>
          <b>Hiroyasu Akada</b>, Jian Wang, Vladislav Golyanik, and Christian Theobalt
          <br>
          <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024, <b style="color:red">Highlight (top
            3.5%)
          </b>
          <br>
          <a href="https://4dqv.mpi-inf.mpg.de/UnrealEgo2/">[Project page]</a>
          <a href="https://unrealego.mpi-inf.mpg.de/">[Benchmark Challenge]</a>
          <a href="https://arxiv.org/abs/2401.00889">[Paper]</a>
          <a href="https://github.com/hiroyasuakada/3D-Human-Pose-Perception-from-Egocentric-Stereo-Videos">[Code]</a>
          <p>
            In this work, we propose a new transformer-based framework to improve egocentric stereo 3D human
            pose
            estimation, which leverages the scene information and temporal context of egocentric stereo videos.
            Furthermore, we introduce two new benchmark datasets, i.e., UnrealEgo2 and UnrealEgo-RW (RealWorld).
            Our extensive experiments show that the proposed approach significantly outperforms previous
            methods.
          </p>
        </td>
      </tbody>


      <tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src="projects/cvpr2024_eventego3d/eventego3d.png" width="180">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href=>
            <h3 class="papertitle">
              <strong>EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams</strong>
            </h3>
          </a>
          Christen Millerdurai, <b>Hiroyasu Akada</b>, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav
          Golyanik
          <br>
          <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024
          <br>
          <a href="https://4dqv.mpi-inf.mpg.de/EventEgo3D/">[Project page]</a>
          <a href="https://arxiv.org/abs/2404.08640">[Paper]</a>
          <a href="https://github.com/Chris10M/EventEgo3D">[Code]</a>
          <p>
            We tackle a new problem, i.e. 3D human motion capture from an egocentric monocular event camera with
            a
            fisheye lens.
            Event streams have high temporal resolution and could provide reliable cues for 3D human motion
            capture under high-speed human motions and rapidly changing illumination.
            We leverage these characteristics and propose the first approach for event-based 3D human pose
            estimation, EventEgo3D (EE3D).
            The proposed EE3D framework is specifically tailored for learning with event streams in the LNES
            representation, enabling high 3D reconstruction accuracy.
            We also provide two new datasets, EE3D-S and EE3D-R.
          </p>
        </td>
      </tbody>


      <tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src="projects/eccv2022_unrealego/overview_of_setting_2-compressed-modified.png" width="180">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://4dqv.mpi-inf.mpg.de/UnrealEgo/">
            <h3 class="papertitle">
              <strong>UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture</strong>
            </h3>
          </a>
          <b>Hiroyasu Akada</b>, Jian Wang, Soshi Shimada, Masaki Takahashi, Christian Theobalt, and Vladislav
          Golyanik
          <br>
          <em>European Conference on Computer Vision (ECCV)</em>, 2022
          <br>
          <a href="https://4dqv.mpi-inf.mpg.de/UnrealEgo/">[Project page]</a>
          <a href="https://arxiv.org/abs/2208.01633">[Paper]</a>
          <a href="https://github.com/hiroyasuakada/UnrealEgo">[Code]</a>
          <p>
            We present UnrealEgo, <i>i.e.</i> a new large-scale naturalistic dataset for egocentric 3D human
            pose
            estimation.
            UnrealEgo is based on an advanced concept of eyeglasses equipped with two fisheye cameras that can
            be
            used in unconstrained environments.
            UnrealEgo is the first dataset to provide in-the-wild stereo images with the largest variety of
            motions among existing egocentric datasets.
            We also propose a new benchmark method that achieves the state-of-the-art results on UnrealEgo.
          </p>
        </td>
      </tbody>



      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src="projects/wacv2022_SSRL/overview.png" width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a
          href="https://openaccess.thecvf.com/content/WACV2022/html/Akada_Self-Supervised_Learning_of_Domain_Invariant_Features_for_Depth_Estimation_WACV_2022_paper.html">
          <h3 class="papertitle">
            <strong>Self-Supervised Learning of Domain Invariant Features for Depth Estimation</strong>
          </h3>
        </a>
        <b>Hiroyasu Akada</b>, Shariq Farooq Bhat, Ibraheem Alhashim, Peter Wonka<br>
        <em>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2022<br>
        <a
          href="https://openaccess.thecvf.com/content/WACV2022/html/Akada_Self-Supervised_Learning_of_Domain_Invariant_Features_for_Depth_Estimation_WACV_2022_paper.html">[Paper]</a>
        <a
          href="https://github.com/hiroyasuakada/Self-Supervised-Learning-of-Domain-Invariant-Features-for-Depth-Estimation">[Code]</a>
        <p>
          We tackle the problem of unsupervised synthetic-to-real domain adaptation for single image depth
          estimation.
          An essential building block of single image depth estimation is an encoder-decoder task network that
          takes RGB images as input and produces depth maps as output.
          In this paper, we propose a novel training strategy to force the task network to learn domain
          invariant
          representations in a self-supervised manner.
        </p>
      </td>
      </tbody>



      <tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src="projects/ias2021_unpaired/overview.png" width="180">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://link.springer.com/chapter/10.1007/978-3-030-95892-3_48">
            <h3 class="papertitle">
              <strong>Dynamic Object Removal from Unpaired Images for Agricultural Autonomous Robots</strong>
            </h3>
          </a>
          <b>Hiroyasu Akada</b> and Masaki Takahashi
          <br>
          <em>International Conference on Intelligent Autonomous Systems (IAS)</em>, 2021
          <br>
          <a href="https://link.springer.com/chapter/10.1007/978-3-030-95892-3_48">[Paper]</a>
          <!-- <label class="open" for="pop-up-1"><a>[BibTex]</a></label>
                  <input type="checkbox" id="pop-up-1" style="display: none;">
                  <div class="overlay">
                    <div class="window">
                      <label class="close" for="pop-up-1">×</label>
                      <div class="bibtex" id="bibtex-21-unpaired"></div>
                      <script>
                        var filename = 'https://hakada.github.io/projects/ias2021_unpaired/bibtex.txt';
                        load_text(filename, 'bibtex-21-unpaired');
                      </script>
                    </div>
                  </div> -->
          <p>
            We developed a GAN-based stem that remove dynamic objects in images. The system can be trained without using
            paired images with/without the dynamic objects.
          </p>
        </td>
      </tbody>
    </table>

    <table
      style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Education & Experience</heading>
            <p>
              <b>[Sep 2025 - Present]</b> Internship, <a href="https://research.google/">Google</a> (Zurich,
              Switzerland)<br>
              <b>[Sep 2022 - Present]</b> Ph.D. student, <a
                href="https://www.mpi-inf.mpg.de/departments/visual-computing-and-artificial-intelligence">Max Planck
                Institute for Informatics</a> (Saarbrucken, Germany)<br>
              <b>[July 2021 - Aug 2022]</b> Visiting researcher, <a
                href="https://www.mpi-inf.mpg.de/departments/visual-computing-and-artificial-intelligence">Max Planck
                Institute for Informatics</a> (Saarbrucken, Germany)<br>
              <b>[Apr 2018 - Aug 2022]</b> Master student, <a href="https://www.st.keio.ac.jp/en/">Keio University</a>
              (Kanagawa, Japan) <br>
              <b>[Sep 2020 - June 2021]</b> Internship, <a href="https://www.kaust.edu.sa/en">KAUST</a> (Saudi
              Arabia, remote) <br>
              <b>[July 2019 - Aug 2019]</b> Internship, <a href="https://www.tencent.com/">Tencent </a> (Palo
              Alto, CA, USA) <br>
              <b>[Aug 2018 - May 2019]</b> Student, <a href="https://haas.berkeley.edu/">University of California,
                Berkeley</a>
              (Berkeley, CA, USA) <br>
              <b>[Apr 2014 - Mar 2018]</b> Bachelor student, <a href="https://www.st.keio.ac.jp/en/">Keio University</a>
              (Kanagawa, Japan) <br>
            </p>
          </td>
        </tr>
      </tbody>
    </table>

    <table
      style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Awards & Grants</heading>
            <p>

            <ul>
              <li>
                Nakajima Foundation (中島記念国際交流財団), 2022-2027<br>
                <b style="color:red">Scholarship for my PhD study for 5 years</b>, including tuition fees, living
                expenses, stipends, etc.
              </li>
              <li>
                CREST, Japan Science and Technology Agency, 2021-2022<br>
                Financial support for my research outside of Japan for 1 year.
              </li>
              <li>
                TOBITATE, Ministry of Education, Culture, Sports, Science and Technology, Japan 2018-2019<br>
                Scholarship for my study outside of Japan for 1 year.
              </li>
            </ul>
            </p>
          </td>
        </tr>
      </tbody>
    </table>

    <table
      style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Professional Services & Activities</heading>
            <!-- <p>
                <b>Reviewers:</b>
                <ul>
                  <li> European Conference on Computer Vision: ECCV2022</li>
                </ul>
              </p> -->

            <p>
              <b>Lab Visits:</b>
            <ul>
              <!-- &emsp;- 2022: <br> -->
              <li>2021-2022: Prof. <a
                  href="https://www.mpi-inf.mpg.de/departments/visual-computing-and-artificial-intelligence">Christian
                  Theobalt</a> @MPI-INF</li>
              <li>2020-2021: Prof. <a href="https://peterwonka.net/">Peter Wonka</a> @KAUST</li>
            </ul>
            </p>

            <br>

            <p>
              <b>Conference Participation:</b> <br>
            <ul>
              <li>CVPR 2024 (Seattle, USA)</li>
              <li>ECCV 2022 (Tel Aviv, Israel)</li>
              <li>WACV 2022 (Hawaii, USA)</li>
              <li>IAS (Singapore, online due to COVID)</li>
            </ul>

            <br>

            </p>
            <p>
              <b>Reviewer Experience:</b> <br>
            <ul>
              <li>NeurIPS 2025</li>
              <li>ICCV 2025</li>
              <li>CVPR 2025 (<b style="color:red">Outstanding Reviewer</b>)</li>
              <li>WACV 2025</li>
              <li>CVPR 2024</li>
              <li>WACV 2024</li>
            </ul>
            </p>


          </td>
        </tr>
      </tbody>
    </table>
    <table
      style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <!-- <p
                      style="text-align:center;font-size:small;">
                      <script type='text/javascript'
                        id='clustrmaps'
                        src='//cdn.clustrmaps.com/map_v2.js?cl=c9c1c1&w=200&t=n&d=j0j_3rEh4HCvS5DIxlRJ3P9J8yp0mtUeDLZ6Z6I2NJ8&co=ffffff&cmo=ea8d7a&cmn=1772d0&ct=000000'></script>
                    </p> -->
            <p style="text-align:right;font-size:small;">
              © Hiroyasu Akada 2022 /
              Design: <a href="https://github.com/jonbarron/jonbarron_website">jonbarron</a>
              Icons: <a href="https://www.flaticon.com/free-icons/sushi" title="sushi icons">Freepik - Flaticon</a>
            </p>
          </td>
        </tr>
      </tbody>
    </table>
    <!-- <script>
            (function(i, s, o, g, r, a, m) {
                i['GoogleAnalyticsObject'] = r;
                i[r] = i[r] || function() {
                    (i[r].q = i[r].q || []).push(arguments)
                }, i[r].l = 1 * new Date();
                a = s.createElement(o),
                    m = s.getElementsByTagName(o)[0];
                a.async = 1;
                a.src = g;
                m.parentNode.insertBefore(a, m)
            })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

            ga('create', 'UA-162031573-1', 'auto');
            ga('send', 'pageview');
        </script>
          </td>
        </tr>
      </tbody></table><iframe frameborder="0" scrolling="no"
      style="border: 0px; display: none; background-color: transparent;"></iframe>
    <div id="GOOGLE_INPUT_CHEXT_FLAG" input="null"
      input_stat="{&quot;tlang&quot;:true,&quot;tsbc&quot;:true,&quot;pun&quot;:true,&quot;mk&quot;:false,&quot;ss&quot;:true}"
      style="display: none;"></div> -->
</body>

</html>